
# ğŸ“– Part I: Foundations

## Chapter 1: Introduction to Apache Spark

### The Genesis of Spark

**Google's Contributions (2003-2004):**
- Google File System (GFS) - Fault-tolerant distributed filesystem
- MapReduce (MR) - Parallel programming paradigm
- Bigtable - Scalable structured data storage

**Hadoop at Yahoo! (2006):**
- HDFS (Hadoop Distributed File System)
- MapReduce implementation
- **Shortcomings:**
  - Hard to manage and administer
  - Verbose API with boilerplate code
  - Performance overhead (disk I/O between map/reduce stages)
  - Not suitable for ML, streaming, or interactive SQL

```
Map Phase â†’ Disk Write â†’ Reduce Phase â†’ Disk Write â†’ Next Stage
```

### Spark's Early Years at AMPLab (2009)
- 10-20x faster than Hadoop MapReduce
- In-memory storage for intermediate results
- Unified APIs for multiple workloads

### What is Apache Spark?

**Four Key Characteristics:**

| Characteristic | Description |
|----------------|-------------|
| **Speed** | DAG scheduler, Tungsten engine, in-memory computation |
| **Ease of Use** | Simple APIs in Java, Scala, Python, R, SQL |
| **Modularity** | Unified libraries (SQL, Streaming, MLlib, GraphX) |
| **Extensibility** | Connectors to various data sources |

### Spark Components

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Spark SQL                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚        Spark Streaming              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚           MLlib                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚           GraphX                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚      Spark Core Engine              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Spark Distributed Execution

**Components:**
- **Driver** - Orchestrates parallel operations
- **SparkSession** - Unified entry point (Spark 2.0+)
- **Cluster Manager** - Standalone, YARN, Mesos, Kubernetes
- **Executors** - Run tasks on worker nodes

**Deployment Modes:**

| Mode | Driver | Executor | Cluster Manager |
|------|--------|----------|-----------------|
| Local | Single JVM | Same JVM | Same host |
| Standalone | Any node | Each node | Any host |
| YARN (client) | Client (not cluster) | NodeManager's container | ResourceManager |
| YARN (cluster) | YARN Application Master | NodeManager's container | ResourceManager |
| Kubernetes | Pod | Pod | Kubernetes Master |

### Partitions and Parallelism

```
Data on Disk:
â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”
â”‚Part 1â”‚ â”‚Part 2â”‚ â”‚Part 3â”‚ â”‚Part 4â”‚
â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜
    â†“        â†“        â†“        â†“
Executor  Core 1   Core 2   Core 3   Core 4
â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”
â”‚Task 1â”‚ â”‚Task 2â”‚ â”‚Task 3â”‚ â”‚Task 4â”‚
â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜
```

**Code Example - Creating Partitions:**
```python
# Create DataFrame with 8 partitions
df = spark.range(0, 10000, 1, 8)
print(df.rdd.getNumPartitions())  # Output: 8

# Repartition existing DataFrame
log_df = spark.read.text("large_file.txt").repartition(8)
print(log_df.rdd.getNumPartitions())  # Output: 8
```

---
